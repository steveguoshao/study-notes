1.在master上下载

下载的是hadoop-2.7.4.tar.gz，与HBase-1.2.6匹配，放到/usr/local/src/目录下

```
wget --no-cookie --no-check-certificate --header "Cookie:oraclelicense=accept-securebackup-cookie" \
    -P /usr/local/src http://mirrors.hust.edu.cn/apache/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz
```

2.解压

```
cd /usr/local/src/
mkdir /usr/local/hadoop && tar--strip-components=1 –xzvf hadoop-2.7.4.tar.gz –C /usr/local/hadoop
```

3.修改hadoop配置

hadoop一共7个文件需要修改:

```
$HADOOP_HOME/etc/hadoop/hadoop-env.sh
$HADOOP_HOME/etc/hadoop/yarn-env.sh
$HADOOP_HOME/etc/hadoop/core-site.xml
$HADOOP_HOME/etc/hadoop/hdfs-site.xml
$HADOOP_HOME/etc/hadoop/mapred-site.xml
$HADOOP_HOME/etc/hadoop/yarn-site.xml
$HADOOP_HOME/etc/hadoop/slaves
```

进入hadoop配置目录

```
cd /usr/local/hadoop/etc/hadoop
```

* 修改hadoop-env.sh和yarn-env.sh, 修改JAVA\_HOME目录

```
export JAVA_HOME=/usr/local/java/jdk1.8.0_144
```

* 修改core-site.xml

```
<configuration>
  <property>
    <name>fs.defaultFS</name>
    <value>hdfs://master:9000</value>
  </property>
  <property>
    <name>hadoop.tmp.dir</name>
    <value>/usr/local/hadoop/tmp</value>
  </property>
</configuration>
```

core-site.xml的完整参数请参考[http://hadoop.apache.org/docs/r2.6.2/hadoop-project-dist/hadoop-common/core-default.xml](http://hadoop.apache.org/docs/r2.6.2/hadoop-project-dist/hadoop-common/core-default.xml)

* 修改hdfs-site.xml

```
<configuration>
   <property>
    <name>dfs.datanode.ipc.address</name>
    <value>0.0.0.0:50020</value>
  </property>
  <property>
    <name>dfs.datanode.http.address</name>
    <value>0.0.0.0:50075</value>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
</configuration>
```

dfs.replication表示数据副本数，一般不大于datanode的节点数。

hdfs-site.xml的完整参数请参考[http://hadoop.apache.org/docs/r2.6.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml](http://hadoop.apache.org/docs/r2.6.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml)

* 修改mapred-site.xml

```
<configuration>
  <property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
  </property>
</configuration>
```

mapred-site.xml的完整参数请参考[http://hadoop.apache.org/docs/r2.6.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml](http://hadoop.apache.org/docs/r2.6.2/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml)

* 修改yarn-site.xml

```
<configuration>
  <property>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
  </property>
  <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>master</value>
   </property>
</configuration>
```

yarn-site.xml的完整参数请参考[http://hadoop.apache.org/docs/r2.6.2/hadoop-yarn/hadoop-yarn-common/yarn-default.xml](http://hadoop.apache.org/docs/r2.6.2/hadoop-yarn/hadoop-yarn-common/yarn-default.xml)

* 修改slaves

```
vim slaves
slave01
slave02
```

4.初始化namenode

```
./bin/hdfs namenode –format
```

当看到INFO common.Storage: Storage directory /home/hadoop/tmp/dfs/name has been successfully formatted. 就表示格式化成功

5.把配置好的hadoop复制到其他机器

```
scp -r hadoop root@slave01:/usr/local/hadoop/
scp -r hadoop root@slave02:/usr/local/hadoop/
```

6.启动整个集群

在master上的/usr/local/Hadoop/目录下执行：

```
./sbin/start-dfs.sh
```

再执行：

```
jps
```

如果正常的话, 会有SecondaryNameNode,NameNode两个进程

启动yarn

```
./sbin/start-yarn.sh
```

再执行：

```
jps
```

如果正常的话，就会多一个ResourceManager进程

而slave01、slave02的机器上执行`jps`会有DataNode,NodeManager 两个节点

这个时候master上的 50070和 8088 也可以用浏览器访问了: [http://99.48.18.233:50070/](http://master:50070/) [http://master:8088/](http://master:8088/)

